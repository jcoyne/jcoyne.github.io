<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="../css/deck/core/deck.core.css">
    <link rel="stylesheet" href="../css/deck/themes/web-2.0.css">
    <link rel="stylesheet" href="../css/home.css">
    <link rel="stylesheet" href="../css/prism.ruby-default.css">
    <script src="../js/modernizr.custom.js"></script>
  </head>
  <body class="home">

  <article class="deck-container">
    <section class="slide"  style="text-align: center">
      <h2>Deployment with AWS Elastic Container Service</h2>
      <h3>Justin Coyne</h3>
      <h4>Stanford Libraries</h3>
    </section>

    <section class="slide">
      <h2>An abbreviated history of cloud computing</h2>
      <img src="cloud.jpg"  style="height: 60vh" title="cc by-nc-nd by Aristocrat https://www.flickr.com/photos/netphotography/15036969665">

    </section>

    <section class="slide">
      <h2>In the beginning there was the server room</h2>
      <img src="servers.jpg" title="cc by-nc-nd by Berkley Lab https://www.flickr.com/photos/berkeleylab/4157700219">

    </section>

    <section class="slide">
      <h2>Then Jeff Bezos offered to rent you a virtual machine (2006)</h2>
      <img src="bezos.jpg" style="height: 65vh" title="cc by Steve Jurvetson https://www.flickr.com/photos/jurvetson/5129286606">

    </section>

    <section class="slide">
      <h2>and it was good.</h2>
      <img src="squirrel.jpg"  style="height: 70vh" title="CC by-nc loren chipman https://www.flickr.com/photos/97902275@N05/41767706762">
      <aside>There's no longer need to deal with hardware or manage a server room.

        What is our core competency in the library/repo/archive?
        Delivering a repository service: preservation, access, discovery.
        It is not in managing machines.
        Cloud allows us to outsource things we need to do, but aren't part of our focus:
        Negotiating with hardware vendors,
        Payments,
        Finding rack space,
        Installing,
        network connectivity,
        cooling,
        power
        (and managing the hardware life cycle)

        unless you have a legal requirement to have your own hardware,
        or you have some funny accounting where your department is not directly
        paying these costs, it's far more effective to pay AWS to do this work.

      </aside>
    </section>

    <section class="slide">
      <h2>Containers appeared (2008)</h2>
      <img src="docker_logo-1.png" style="height: 60vh" >
      <aside>A container is a set of processes isolated from the rest of the system.
        Allows us to manage a single service apart from the operating system itself.
        Docker is the de facto container standard right now, although there are alternatives.
      </aside>
    </section>

    <section class="slide">
      <h2>Container as a Service (CaaS)</h2>
      <ul style="list-style: none">
        <li style="display: inline-block;"><img src="acs.png" alt="Microsoft Azure Container Service"></li>
        <li style="display: inline-block;"><img src="ecs.png" alt="AWS Elastic Container Service"></li>
        <li style="display: inline-block;"><img src="droplet.svg" alt="DigitalOcean Droplet" style="height: 220px; width: 220px;"></li>
        <li><img src="gke.png" alt="Google Kubernetes Engine"></li>
      </ul>
      <aside>About 3 years back Containers as a Service offerings began showing up.

        These are hosting + orchestration services.  An orchestration service
        organizes the individual containers that make up an application at the networking level.
        Allows us to roll out new versions without disrupting users, scale up, etc.
    </section>


    <section class="slide">
      <h2 style="display:none ">Why use containers?</h2>
      <img style="height: 100vh" src="ken-treloar-346065-unsplash.jpg" title="Photo by Ken Treloar on Unsplash">
    </section>

    <section class="slide">
      <h2>Consistent environment</h2>
      <img src="consistency.jpg"  title="cc by-nc-nd by Adam Vogel https://www.flickr.com/photos/newt0/6614704109">
      <aside>
        Containers give a great deal of power and responsibility to the developer.
        You can run the same container image on your laptop as you deploy in production.

        Far fewer "Works for me" issues and calls from ops team saying "why doesn't it deploy?"
      </aside>
    </section>

    <section class="slide">
      <h2>Isolation</h2>
      <img src="isolation.jpg" style="height: 70vh" title="cc by-nc by David Ingram https://www.flickr.com/photos/dingatx/8219989409">

      <aside>
      Where on a virtual machine, the application might be dependent on the host operating system,
      with containser, we're making all of our dependencies very clear.

      This makes it possible to prevent problems like when database uses up all the memory and crashing the webserver.
      Each of these processes is now operating in their own container, with their own resource allotment.
      </aside>
    </section>

    <section class="slide">
      <h2>Run Anywhere</h2>
      <img src="run.jpg" style="height: 70vh" title="cc by-nc-nd by Mina Guli https://www.flickr.com/photos/run4water/33318217996">

      <aside>
        The container images provide portability and version control.

        We can run all the containers on one machine, run multiple copies of one image.
        If one of our containers is using too many resources we can move that container to a different instance.
      </aside>
    </section>

    <section class="slide">
      <h2>Better deployment</h2>
      <img src="ankush-minda-549369-unsplash.jpg" style="height: 70vh" title="Photo by Ankush Minda on Unsplash">
      <aside>
        I argue that pushing a container image is currently the best way to deploy a service.

        Nothing to do on the server other than run the image and provide configuration
        (typically via environment variables).

        So, hopefully, you now understand why containers seem to be the future.
      </aside>
    </section>


    <section class="slide">
      <h2>Amazon Elastic Container Service (ECS)</h2>
      <aside>
        At Stanford we've been using ECS for over a year now and we're pretty
        happy with out decision. I'm going to walk through what we do to spin this up.
      </aside>
    </section>


    <section class="slide">
      <img src="choice.png">

      <aside>
        At Stanford we've been using ECS for over a year now and we're pretty
        happy with out decision. I'm going to walk through what we do to spin this up.

        ECS has two modes, Fargate launch type and EC2 launch type.

        With Fargate launch type, all you have to do is package your application
        in containers, specify the CPU and memory requirements,
        define networking and IAM policies, and launch the application.

        EC2 launch type allows you to have server-level, more granular control
        over the infrastructure that runs your container applications. With EC2
        launch type, you can use Amazon ECS to manage a cluster of ec2 instances and
        schedule placement of containers on the servers. You are responsible for
        provisioning, patching, and scaling clusters of ec2 instances.

        We've found that for our needs Fargate is far easier to set up and doesn't
        have instances that we have to patch. It allows us to focus on delivering
        value to the user.
      </aside>
    </section>

    <section class="slide">
      <img src="network.png">
    </section>

    <section class="slide">
      <img src="launch.png">
    </section>

    <section class="slide">
      <img src="task.png">
    </section>

    <section class="slide">
      <img src="container.png">
    </section>

    <section class="slide">
      <img src="launch-task.png">
    </section>

    <section class="slide">
      <img src="service1.png">
    </section>

    <section class="slide">
      <img src="service2.png">
    </section>

    <section class="slide">
      <img src="service3.png">
    </section>

    <section class="slide">
      <img src="service4.png">
    </section>

    <section class="slide">
      <img src="launch-service.png">
    </section>

    <section class="slide">
      <img src="logs.png">
    </section>

    <section class="slide">
      <img src="port.png">
    </section>

    <section class="slide">
      <img src="rails.png">
      <aside>
        And that's it! You're up and running.

        At Stanford, we prefer to not click through this stuff, we use Terraform
        to build this, but we *could* build it this way if we wanted to.
        We only prefer to keep our infrastructure configuration managed in Git.

        There are a few other steps we may have used in production, such as setting
        up an Application load balancer to do port forwarding, but thats orthagonal
        to actually using containers.
      </aside>
    </section>

    <section class="slide">
      <h2>AWS Terms</h2>
      <ul>
        <li>Cluster - e.g.: The production environment</li>
        <li>Task - Which containers should I run and how many resources</li>
        <li>Service - Network, Load balance, Auto scale</li>
      </ul>
      <aside>
      </aside>
    </section>

    <section class="slide">
      <h2>Cost</h2>
       <p class="slide">The price per vCPU is $0.00001406 per second ($36.43 per month) and
         per GB memory is $0.00000353 per second ($9.14 per month).</p>

        <p class="slide">~$600 per year</p>

        <p class="slide">You can turn it off when you're not using it!</p>

        <p class="slide">Set up autoscaling to add capacity when you need it and scale back when you don't.</p>
      <aside>
      </aside>
    </section>

    <section class="slide">
      <h2>Hyrax's big flaw</h2>
      <img src="hyrax-deploy.svg" style="height: 70vh; width: 80vw;">
      <aside>
        A long time ago, I was working with PSU and they had some capacity problems.
        I helped them make the expedient decision that they could scale out their
        Sufia by just making sure all the nodes had a shared file system.
        This worked well at the time, but requires a bit
        of work by whomever manages your servers.  In retrospect, this was a bad
        decision for the community as the shared file system approach makes Hyrax
        hard to deploy in the cloud on a scalable infrastructure.
      </aside>
    </section>

    <section class="slide">
      <h2>The Hyku Solution</h2>
      <img src="hyku-deploy.svg" style="height: 70vh; width: 80vw;">
      <aside>
        When we started the Hyku project we hit this shared volume hurdle again.
        We patched Hyrax so that it can use carrierwave-aws to store files on S3.

        Unfortunately this is not the default, and this ties us to AWS.
        In the future, I'd like to see the Hyrax road map consider ActiveStorage for
        managing uploads.
      </aside>
    </section>


    <section class="slide">
      <h2>How can we make Hyrax make better use of containers?</h2>
      <aside>
        I see Valkyrie playing an important role here -- mediating between services and
        the application layer.

        Explore ElasticSearch (instead of Solr)

        I think we should increase our investment in Hyku (or another "ready to go" solution)
        and encourage people to start there. Hyku already has Docker compose files
        and Cloud Formation scripts. Currently these are targeted at a service provider, but
        there's no reason these couldn't be standalone.

        I'd like to see Docker being the default way to deploy Hyrax, even if you don't use ECS
      </aside>
    </section>

    <section class="slide">
      <h1>Fin.</h1>
    </section>


  </article> <!-- deck-container -->
  <p class="deck-status" aria-role="status">
    <span class="deck-status-current"></span>
    /
    <span class="deck-status-total"></span>
  </p>

  <script src="../js/jquery.min.js"></script>

  <script src="../js/deck/core/deck.core.js"></script>
  <!-- <script src="./js/deck/extensions/goto/deck.goto.js"></script> -->
  <!-- <script src="./js/deck/extensions/navigation/deck.navigation.js"></script> -->
  <script src="../js/deck/extensions/status/deck.status.js"></script>
  <!-- <script src="./js/deck/extensions/hash/deck.hash.js"></script> -->
  <script src="../js/prism.ruby-default.js"></script>
  <script src="../js/home.js"></script>
  </body>
</html>
